\subsection{Cost Function}

Loss or cost function, also known as the objective function, is a fundamental component in the training process of neural networks. It quantifies the discrepancy or error between the predicted outputs of the network and the true target values, serving as a measure of how well the network is performing.
The cost function plays a crucial role in training by providing a quantitative measure of the network's performance, enabling the optimization algorithms to adjust the network's parameters, typically the weights, to minimize the cost. One commonly used optimization algorithm is gradient descent, which relies on the differentiability of the cost function to compute the gradients and update the weights iteratively. The network aims to reduce the prediction error and improve its performance on unseen data by minimizing the cost function over the training data.
However, it is important to note that not all cost functions are differentiable. In certain cases, non-differentiable cost functions may be encountered, which poses challenges for optimization algorithms that rely on gradient-based methods. Non-differentiability can occur when the cost function involves non-smooth operations, such as max or absolute value functions. In such situations, alternative optimization techniques, such as subgradient methods or heuristics, may be employed to approximate the optimal solution.
Over the years, various objective functions have been proposed and studied, each with its own characteristics and suitability for different tasks. For classification tasks, common choices of cost functions include cross-entropy, Mean Squared Error (MSE), and Mean Absolute Error (MAE). These functions have shown promising results and have been extensively used in neural network research.
