\subsection{Back-propagation}

To extend the perceptron model's learning capabilities, the concept of backpropagation was introduced. This algorithm, initially proposed by Paul Werbos in 1974, revolutionized neural network training by efficiently propagating error gradients backward through the layers of the network. By iteratively adjusting the weights based on these gradients, backpropagation enabled neural networks to learn complex patterns and improve their performance.