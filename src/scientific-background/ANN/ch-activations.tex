\subsection{Activation functions}

The perceptron model originally employed a single layer with a threshold activation function, which limited its ability to handle non-linearly separable problems. However, the introduction of various activation functions over the years, such as the Rectified Linear Unit (ReLU), parametric ReLU, hyperbolic tangent (tanh), and sigmoid functions, played a crucial role in neural networks by introducing non-linearities to the output of each neuron. The concept of activation functions in neural networks traces back to the early foundation of the field. While there have been numerous contributions and refinements, the incorporation of activation functions can be attributed to Warren McCullen and Walter Pits in the 1940s \cite{mcculloch1943logical}. They introduced the Heaviside activation function, which later inspired the development of ReLU and its variants.
By using activation functions, neural networks gained the ability to model complex and nonlinear relationships. This allowed the construction of deeper architectures with multiple layers, enabling networks to learn hierarchical representations and capture intricate features in the input data.
These advancements in backpropagation and activation functions have played a crucial role in the development of neural networks. They have enabled the training of deeper, more complex architectures and facilitated the modeling of non-linear relationships, making neural networks more powerful and versatile in solving various problems.
