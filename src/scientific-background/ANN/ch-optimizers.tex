\subsection{Optimizers}

Optimizers play a crucial role in effectively training neural network models, and numerous approaches and algorithms have been proposed in recent years. Among the approaches employed in our work, Stochastic Gradient Descent (SGD) is the primary optimization technique utilized. This approach involves the partitioning of the training data into smaller randomly selected subsets. By iterating over these subsets, the model's parameters are updated with respect to the examples within each subset, facilitating the learning process.
Algorithmic optimizers, on the other hand, represent another category of optimizers that dynamically compute the learning rate based on current and past gradients. Some well-known optimizers in this category include Momentum, AdaGrad, and ADAM. These optimizers have gained recognition for their ability to enhance training efficiency and convergence in neural network models.
