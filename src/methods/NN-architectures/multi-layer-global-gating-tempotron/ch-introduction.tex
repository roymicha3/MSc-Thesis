\subsubsection{Introduction}

We can easily expand the equation to multilayer networks by adding a new index $l \in [L]$ representing the layer – in two ways:

\begin{enumerate}
    \item Gating integrated in the weights:


\begin{equation}
    W_{\text{eff}}^l = G^t(x) \cdot W^l \in \mathbb{R}^{N_l \times N_{l-1}}, \quad N_l \text{ is the dimensions of the } l\text{'th layer}
\end{equation}

For all $l \in [L]$, we have $z^l = W_{\text{eff}}^l \cdot z^{l-1} \in \mathbb{R}^{N_l}$.

where:
$z^0$ represents the input voltage $x \in \mathbb{R}^{N_0}$
\begin{equation}
    t^k_{max} = \underset{t}{argmax} \left( 
    output^L[k](t)
    \right)
\end{equation}


and ${\text{output}} = \text{Softmax}\left(\begin{bmatrix} z^L[1](t_{\text{max}}^1) \\ \vdots \\ z^L[N_L](t_{\text{max}}^{N_L}) \end{bmatrix}\right)$.
    \item Global Gating as an activation function:

For each input sample $\mu$, $f_\mu^m(z) = g_m(x_\mu) \cdot z$,
where $W_m^l \in \mathbb{R}^{N_l \times N_{l-1}}$ and $N_l$ is the dimensions of the $l$'th layer.
For all $l \in [L]$, we have $z^l = \sum_{m=1}^M W_m^l \cdot f_\mu^m(z^{l-1}) = \sum_{m=1}^M f_\mu^m(W_m^l \cdot z^{l-1}) \in \mathbb{R}^{N_l}$.

\end{enumerate}

For Boolean gating, we will use the first definition, and for time-dependent gating, we will choose the second.

Now that we have our definitions – which are the same definitions for a fully connected multilayered network – we can look at the training procedure of a regular fully connected network and adapt it to our variables.
