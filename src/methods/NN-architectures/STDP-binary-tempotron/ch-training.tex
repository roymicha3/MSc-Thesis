\subsubsection{Training - Simple Cost function}

\begin{mdframed}[backgroundcolor=red_background, linecolor=black, linewidth=2pt, frametitle=\textbf{Statement}]
\begin{center}

    \label{st:GD-simple-medel}
    Using the Gradient Descent method our step iteration will be:
    \begin{equation}
        \Delta w_j = -y \cdot H(-y \cdot f(v_{max})) \cdot K_j(t_{\text{max}})
    \end{equation}

\end{center}
\end{mdframed}

\textbf{Proof:}

Lets first define:
\begin{equation} \notag
    a = v_{max}-v_{th} \Rightarrow \frac{\partial{a}}{\partial{w_j}} = \frac{\partial{v_{max}}}{\partial{w_j}} = K_j(t_{max})
\end{equation}
\begin{equation} \notag
    z = -y \cdot a \Rightarrow \frac{\partial{z}}{\partial{a}} = -y
\end{equation}
\begin{equation} \notag
    c = H(z) \cdot z \Rightarrow \frac{\partial{c}}{\partial{z}} = H(z)
\end{equation}
And overall, using the chain rule:
\begin{equation}
    \frac{\partial{c}}{\partial{w}} = \frac{\partial{c}}{\partial{z}} \cdot \frac{\partial{z}}{\partial{a}} \cdot \frac{\partial{a}}{\partial{w}} = H(z) \cdot (-y) \cdot K_j(t_{max}) = -y \cdot H(-y \cdot f(v_{max})) \cdot K_j(t_{\text{max}})
\end{equation}

\begin{mdframed}[backgroundcolor=green_background, linecolor=black, linewidth=2pt, frametitle=\textbf{Pseudo-code}]

For each sample and label \(x, y\) and learning rate \(\gamma\): \\
In each iteration of the learning algorithm, we will need to:

\begin{enumerate}

    \item Evaluate $V[t]$ by: 

\begin{equation}
    V[t] = \sum_{j} w_j \cdot k_j[t]
\end{equation}

    \item Find $t_{\text{max}}$ by: 

\begin{equation}
    t_{\text{max}} = \arg\max_t V[t]
\end{equation}

    \item Finally, compute $\Delta w_j = -y \cdot H(-y \cdot f(v_{max})) \cdot K_j[t_{\text{max}}]$

\end{enumerate}
\end{mdframed}

\subsubsection{Training - Convolution based Cost function}


\begin{mdframed}[backgroundcolor=red_background, linecolor=black, linewidth=2pt, frametitle=\textbf{Statement}]
\begin{center}

    \label{st:GD-convl-medel}
    Using the Gradient Descent method our step iteration will be:

    \begin{equation}
        \Delta w_j = -y \cdot H(-y \cdot f(h_{\mu)})) \cdot \frac{\int e^{\beta v_{\mu}(t)} k_i(t) dt}{\int e^{\beta v_{\mu}(t)} dt}
    \end{equation}

\end{center}
\end{mdframed}

\textbf{Proof:}

In the next computations, we will use the following derivative:
\begin{equation}
    \frac{dh_{\mu}}{dw_i} = \frac{d}{dw_i}\ln\left(\int e^{\beta v_{\mu}(t)} dt\right) = \frac{1}{\int e^{\beta v_{\mu}(t)} dt} \cdot \frac{d}{dw_i}\int e^{\beta v_{\mu}(t)} dt
\end{equation}

\begin{equation} \notag
= \frac{1}{\int e^{\beta v_{\mu}(t)} dt}\int e^{\beta v_{\mu}(t)} \cdot \frac{d}{dw_i} \cdot \beta v_{\mu}(t) dt = \frac{1}{\beta}\sum_{\mu}{y_{\mu}\frac{1}{\int e^{\beta v_{\mu}(t)} dt}\int e^{\beta v_{\mu}(t)} \beta k_i(t) dt}
\end{equation}

\begin{equation} \notag
= \beta \cdot \frac{\int e^{\beta v_{\mu}(t)} k_i(t) dt}{\int e^{\beta v_{\mu}(t)} dt}
\end{equation}

Lets first define:
\begin{equation} \notag
    a = h_{\mu} - v_{th} \Rightarrow \frac{\partial{a}}{\partial{w_j}} = \frac{\partial{h_{\mu}}}{\partial{w_j}} = \beta \cdot \frac{\int e^{\beta v_{\mu}(t)} k_i(t) dt}{\int e^{\beta v_{\mu}(t)} dt}
\end{equation}
\begin{equation} \notag
    z = -y \cdot a \Rightarrow \frac{\partial{z}}{\partial{a}} = -y
\end{equation}
\begin{equation} \notag
    c = H(z) \cdot z \Rightarrow \frac{\partial{c}}{\partial{z}} = H(z)
\end{equation}
And overall, using the chain rule:
\begin{equation}
    \frac{\partial{c}}{\partial{w}} = \frac{\partial{c}}{\partial{z}} \cdot \frac{\partial{z}}{\partial{a}} \cdot \frac{\partial{a}}{\partial{w}} = H(z) \cdot (-y) \cdot \beta \cdot \frac{\int e^{\beta v_{\mu}(t)} k_i(t) dt}{\int e^{\beta v_{\mu}(t)} dt} = -y \cdot H(-y \cdot f(h_{mu})) \cdot \beta \cdot \frac{\int e^{\beta v_{\mu}(t)} k_i(t) dt}{\int e^{\beta v_{\mu}(t)} dt}
\end{equation}

That gives us the new step in our iterations:

\begin{equation}
\Delta w_i =- \beta \cdot y \cdot H(-y \cdot f(h_{mu})) \cdot \frac{\int e^{\beta v_{\mu}(t)} k_i(t) dt}{\int e^{\beta v_{\mu}(t)} dt}
\end{equation}

To talk about the numeric evaluation of $\Delta w$, we first need to talk about numeric integration â€“ using the trapezoidal rule:

\newtheorem{theorem}{Theorem}

\begin{mdframed}[linewidth=2pt, frametitlerule=true, frametitlebackgroundcolor=gray!20, innertopmargin=10pt, innerbottommargin=10pt]
    \begin{theorem}[Trapezoidal Rule]
        For $f(x)$:
        
        \begin{equation*}
            \int_a^b f(x) dx = \sum_{t=a}^{b-\Delta} \int_t^{t+\Delta} f(x) dx
        \end{equation*}

        \begin{equation*}
            \lim_{{\Delta\to0}} \int_t^{t+\Delta} f(x) dx = \Delta \left(\frac{f(t)+f(t+\Delta)}{2}\right)
        \end{equation*}

        For $\Delta = \frac{T}{N}$:

        \begin{equation*}
            \int_0^T f(x) dx = \sum_{n=0}^{N-1} \frac{T}{N}\left(\frac{f[n\frac{T}{N}]+f[(n+1)\frac{T}{N}]}{2}\right)
        \end{equation*}

        Overall, if we assume $f[0] \cdot \frac{T}{N}$ and $f\left[\frac{(N-1)T}{N}\right] \cdot \frac{T}{N}$ are negligible, we get:

        \begin{equation*}
            \int_0^T f(x) dx \approx \frac{T}{N}\sum_{n=0}^{N-1} f[n\frac{T}{N}]
        \end{equation*}
    \end{theorem}
\end{mdframed}

The new step computation will be:

\begin{equation}
    \Delta w_i = y_{\mu} \cdot \frac{\frac{T}{N}\sum_{n=0}^{N-1} e^{\beta \cdot v[nT/N]} \cdot k_i[nT/N]}{\frac{T}{N}\sum_{n=0}^{N-1} e^{\beta \cdot v[nT/N]}} = y_{\mu} \cdot \frac{\sum_{n=0}^{N-1} e^{\beta \cdot v[nT/N]} \cdot k_i[nT/N]}{\sum_{n=0}^{N-1} e^{\beta \cdot v[nT/N]}}
\end{equation}

