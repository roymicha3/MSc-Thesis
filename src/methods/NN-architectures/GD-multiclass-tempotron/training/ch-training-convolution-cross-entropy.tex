\subsubsection{Training â€“ Convolution Based Cross-Entropy}

\begin{mdframed}[backgroundcolor=red_background, linecolor=black, linewidth=2pt, frametitle=\textbf{Statement}]
\begin{center}

    \label{st:GD-cross-entr-convl-tempotron}
    Using the Gradient Descent method our step iteration will be:
    \begin{equation}
        \Delta w_{i,k} = -\beta \cdot (\sigma_k^{\mu} - y_k^{\mu}) \cdot \frac{1}{\int_0^T \exp(\beta v_k(t)) dt} \cdot \int_0^T \exp(\beta v_k(t)) x_i(t) dt
    \end{equation}

\end{center}
\end{mdframed}


Define: 

\begin{equation}
    z_k(x) = \int_0^T \exp(\beta v_k(t)) dt \implies h_k(x) = \ln(z_k(x))
\end{equation}

The derivative is:

\begin{equation}
    \frac{d}{dw_{i,k}} \int_0^T \exp(\beta v_k(t)) dt = \int_0^T \frac{d}{dv_k(t)} \exp(\beta v_k(t)) \cdot \frac{dv_k(t)}{dw_{i,k}} dt = \int_0^T \beta \exp(\beta v_k(t)) \cdot x_i(t) dt
\end{equation}

\begin{equation}
    \frac{dh_k(x)}{dw_{i,k}} = \frac{dh_k(x)}{dz_{i,k}} \cdot \frac{dz_k}{dw_{i,k}} = \frac{1}{z_k} \int_0^T \beta \exp(\beta v_k(t)) x_i(t) dt
\end{equation}

The activation function of the output layer will be the SoftMax function:

\begin{equation}
    p_{\mu}(K_{\mu}=k) = \sigma_k^{\mu} = \frac{\exp(\beta h_k)}{\sum_{k'} \exp(\beta h_{k'})}
\end{equation}

The cross-entropy loss:

\begin{equation}
    E(\mathbf{w}) = -\sum_{\mu} \sum_{k=1}^K y_k^{\mu} \ln(\sigma_k^{\mu})
\end{equation}

\(y_k^{\mu}\) is a one hot vector identifying the label of the \(\mu\)-th sample.

Another useful definition will be:

\begin{equation}
    u_k = \frac{\exp(h_k)}{\sum_{k'} \exp(h_{k'})}
\end{equation}

The derivative is: \\
\begin{itemize}
    \item For \(k'=k\):

\begin{equation}
    \frac{du_k}{dh_k} = \frac{\exp(h_k)}{\sum_{k'} \exp(h_{k'})} - \frac{\exp(h_k)}{\left(\sum_{k'} \exp(h_{k'})\right)^2} \cdot \exp(h_k) = u_k - [u_k]^2
\end{equation}

\item And for \(k' \neq k\):

\begin{equation}
    \frac{du_{k'}}{dh_k} = -\frac{\exp(h_{k'})}{\left(\sum_{k''} \exp(h_{k''})\right)^2} \cdot \exp(h_k) = -u_k \cdot u_{k'}
\end{equation}
\end{itemize}

According to the chain rule:

\begin{equation}
    \frac{d}{dw_{i,k}} E(\mathbf{w}) = -\frac{d}{du_{k'}} \sum_{\mu} \sum_{k'=1}^K y_{k'}^{\mu} \ln(u_{k'}) \cdot \frac{du_{k'}}{dh_k} \cdot \frac{dh_k}{dw_{(i,k)}}
\end{equation}

The chain rule yields:

\begin{equation}
    \frac{d}{du_{k'}} \ln(u_{k'}) \cdot \frac{du_{k'}}{dh_k} \cdot \frac{dh_k}{dw_{i,k}} = \begin{cases}
    \frac{1}{u_{k'}} \cdot (u_k - [u_k]^2) \cdot \frac{dh_k}{dw_{i,k}}, & \text{if } k' = k, \\
    -\frac{1}{u_{k'}} \cdot (u_k \cdot u_{k'}) \cdot \frac{dh_k}{dw_{i,k}}, & \text{if } k' \neq k.
    \end{cases}
\end{equation}

The expression becomes:

\begin{equation}
    \frac{d}{dw_{i,k}} E(\mathbf{w}) = -\sum_{\mu} \sum_{k'=1}^K y_{k'}^{\mu} \cdot (u_k - \delta_{k',k}) \cdot \frac{dh_k}{dw_{i,k}}
\end{equation}

Now, during each iteration on sample \(\mu\) of the learning procedure, we will compute:

\begin{equation}
    \Delta w_{i,k} = -\beta \cdot (\sigma_k^{\mu} - y_k^{\mu}) \cdot \frac{1}{\int_0^T \exp(\beta v_k(t)) dt} \cdot \int_0^T \exp(\beta v_k(t)) x_i(t) dt
\end{equation}
