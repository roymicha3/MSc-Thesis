\chapter{Motivation}
\label{chap:motivation}

The scientific background highlights the growing interest within the scientific community towards computational neuron models, which offer a more biologically plausible approach to modeling neural systems. Furthermore, the development of Neuromorphic hardware can significantly enhance the efficiency and effectiveness of these models.
Our objective is to explore fundamental methods and algorithms employed in the Percep-
tron model context and adapt them to the Tempotron. We aim to gain insights into their rela-
tive strengths and limitations by comparing these models across various tasks and method-
ologies. Through this exploration, we strive to contribute to the broader understanding of
the capabilities and applicability of these models on the Tempotron.

For example:
Let $\overline{x} \in \mathbb{R}^N$ be an input vector, marking the starting point of our investigation into the gating and activation functions. 

\textbf{Definitions:}
\begin{equation}
g_m(\overline{x}) = \theta(\overline{w}_m^g \cdot \overline{x}) \, , \, \overline{w}_m^g \in \mathbb{R}^N \, , \, \theta \in \{\text{soft/hard heaviside, softmax, one-hot vector}\}
\end{equation}
\begin{equation}
    g : \mathbb{R}^N \rightarrow [0, 1]
\end{equation}
\begin{equation}
F_{g_m} : \mathbb{R} \rightarrow \mathbb{R} , \, F_{g_m}(x) = g_m(\overline{x}) \cdot x
\end{equation}

\textbf{Statement:}
From our study, we can easily conclude that the ReLU function is a specific case of the Global Gating mechanism.

\textbf{Proof:}
Setting the stage with $M=N$ and $\overline{w}_m^g = \mathbb{1}_m$, we find a mathematical representation where $g_m(\overline{x}) = H(x_m)$. Delving deeper:
\begin{equation}
w_{i, j}^m = 
\begin{cases}
    w_{i, j} & \text{if } m = j \\
    0 & \text{otherwise}
\end{cases}
\end{equation}
we arrive at a conclusive formula:
\begin{equation}
x_i^{out} = \sum_{j=1}^N w_{i, j} \cdot \text{ReLU}(x_j)
\end{equation}
